## System Design Basics

Investing in scaling before it is needed is generally not a smart business proposition. However, some forethought into the design can save valuable time and resources in the future. 


## Key Characteristics of Distributed Systems

### Scalability

Scalability is the capability of a system to grow and manage increased demand. Distributed systems that can continuously evolve in order to support the growing amount of work is considered to be scalable.

 - **Horizontal Scaling**: scale by adding more servers into your pool of resources. It is often easier to scale dynamically by adding more machines into the existing pool. E.g. [Cassandra](https://en.wikipedia.org/wiki/Apache_Cassandra) and [MongoDB](https://en.wikipedia.org/wiki/MongoDB) provide an easy way to scale horizontally by adding more machines to meet growing needs.

 - **Vertical Scaling**: scale by adding more power (CPU, RAM, Storage, etc.) to an existing server. It is usually limited to the capacity of a single server and scaling beyond that capacity often involves downtime and comes with an upper limit. E.g. MySQL.



### Reliability

 A distributed system is considered reliable if it keeps delivering its services even when one or several of its software or hardware components fail. A reliable distributed system achieves this through redundancy of both the software components and data. Obviously, redundancy has a cost and a reliable system has to pay that to achieve such resilience for services by eliminating every single point of failure.



### Availability

It is a simple measure of the percentage of time that a system, service, or a machine remains operational under normal conditions.

#### Reliability Vs. Availability

If a system is reliable, it is available. However, if it is available, it is not necessarily reliable. High reliability contributes to high availability, but it is possible to achieve a high availability even with an unreliable product by minimizing repair time and ensuring that spares are always available when they are needed. 


### Efficiency

Two standard measures of a system's efficiency are the response time and the throughput.


### Serviceability or Manageability

Another important consideration while designing a distributed system is how easy it is to operate and maintain. Serviceability or manageability is the simplicity and speed with which a system can be repaired or maintained.


## Load Balancing


The reason why we need load balancer includes,

 - it helps to spread the traffic across a cluster of servers
 - it keeps track of the status of all the resources while distributing requests

Load balancers usually sit between the client and the server, accepting incoming requests and distributing the traffic across multiple backend servers using various algorithms, thus a load balancer reduces individual server load and prevents any one application server from becoming a single point of failure.

To utilize full scalability and redundancy, we can try to balance the load at each layer of the system. 

### Benefits of Load Balancing

 - Users requests are immediately passed on to more readily available resources, instead of waiting for a single server to finish its previous tasks.
 - Less downtime and higher throughput.
 - Smart load balancer provides additional features, like predictive analytics that determine traffic bottlenecks before they happen.

### Load Balancing Algorithms

Load balancers consider two factors before forwarding a request to a backend server,

 - first ensure that the server they choose is actually responding appropriately to requests
 - use a pre-configured algorithm to select one from the set of healthy servers

To decide if a backend server is healthy or not, health check is conducted. "Health checks" regularly attempt to connect to backend servers to ensure that servers are listening.

A range of load balancing methods that are being used,

- Least Connection Method: directs traffic to the server with the fewest active connections
- Least Response Time Method: directs traffic to the server with the fewest active connections and the lowest average response time
- Least Bandwidth Method: selects the server that is currently serving the least amount of traffic
- Round Robin Method: cycles through a list of servers and sends each new request to the next server
- Weighted Round Robin Method: designed to better handle servers with different processing capacities
- IP Hash: a hash of the IP address of the client is calculated to redirect the request

### Redundant Load Balancers

A second load balancer can be connected to the first to form a cluster, to remove single point of failure.

## Caching

Caches take advantage of the locality of reference principle: recently requested data is likely to be requested again, and it will enable you to make vastly better use of the resource you already have. 

### Application server cache

Placing a cache directly on a request layer node enables the local storage of response data. However, if the request layer is expanded to multiple nodes, and the load balancer randomly distributes requests across the nodes, the same requests go to different nodes, thus increase cache misses. 

A solution would be global caches or distributed caches.

### Content Delivery (or Distribution) Network (CDN)

CDNs are a kind of cache that comes into play for sites serving large amounts of static media. It speeds up the page loading by hosting the static media closer to the clients. A normal setup is, the clients would request static assets from a CDN server that is close to them. If the resource can be found locally, it will be served. Otherwise, the CDN server would send the request to backend server for the client, then send it to the client and cache it locally.


### Cache Invalidation

While caching can speed up read requests, it requires maintenance to keep the cache and DB consistent. When data is updated in DB, Relevant cache needs to be invalidated.

 - Write-through cache: data is written into the cache and DB simultaneously, nothing will be lost in case of crash or other disruptions, user might experience higher latency for write requests.

 - Write-around cache: data is written into DB, bypassing the cache, it can reduce the cache being flooded with write requests (if writing data that is not likely to be read later, write-through schema will create cache entries anyway), but a write to recently read data would probably create a cache miss.

 - Write-back cache: data is written to cache alone, and completion is immediately confirmed to the client, write to DB is done later, it has low-latency and high-throughput for write intensive applications, but also has the risk of data loss in case of crash.

### Cache eviction policies

 - First in first out (FIFO)
 - Last in first out (LIFO)
 - Least recently used (LRU)
 - Most recently used (MRU)
 - Least frequently used (LFU)
 - Random replacement (RR)


## Data Partitioning

Data partitioning is a technique to break up a big database (DB) into many smaller parts. The justification for data partitioning is that, after a certain scale point, it is cheaper and more feasible to scale horizontally by adding more machines than to grow it vertically by adding beefier servers.

### Partitioning Methods

#### Horizontal partitioning

 1. put different rows into different tables
 2. also called range based partitioning or data sharding
 3. might lead to unbalanced servers if the range used for partitioning is not chosen carefully, e.g. some rows might be hotter and requested more frequently

#### Vertical partitioning

 1. divide data to store tables related to a specific feature in their own server
 2. straightforward to implement and has low impacts on the application

#### Directory based partitioning

 1. create a lookup service which knows your current partitioning scheme and abstracts it away from the DB access code
 2. can perform tasks like adding servers to the DB pool or changing our partitioning scheme without having an impact on the application

### Partitioning Criteria

#### Key or hash-based partitioning

 1. apply a hash function to some key attributes of the entity we are storing, that yields the partition number
 2. it effectively fixes the total number of DB servers, which means it requires redistribution and downtime when changing the hash function or adding new servers
 3. a workaround is to use consistent hashing

#### List partitioning

 1. each partition is assigned a list of values
 2. when inserting a new record, we check which partition contains the key of the new record and store it there

#### Round-robin partitioning

Simple strategy that ensures uniform data distribution.

#### Composite partitioning

We combine any of the above partitioning schemes to devise a new scheme.

### Common Problems of Data Partitioning

On a partitioned database, there are certain extra constraints on the different operations that can be performed.

#### Joins and denormalization

 1. it is often not feasible to perform joins that span database partitions
 2. a common workaround for this problem is to denormalize the database so that queries that previously required joins can be performed from a single table

#### Referential integrity

 1. trying to enforce data integrity constraints such as foreign keys in a partitioned database can be extremely difficult
 2. most of RDBMS do not support foreign keys constraints across databases on different database servers
 3. applications that require referential integrity on partitioned databases often have to enforce it in application code

#### Rebalancing

 1. there could be many reasons we have to change our partitioning scheme
 2. rebalancing without incurring downtime is extremely difficult
 3. directory based partitioning does make rebalancing more palatable, but increases the complexity of the system and a new single point of failure (e.g. lookup service)