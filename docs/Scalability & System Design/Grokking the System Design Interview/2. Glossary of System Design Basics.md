## System Design Basics

Investing in scaling before it is needed is generally not a smart business proposition. However, some forethought into the design can save valuable time and resources in the future. 


## Key Characteristics of Distributed Systems

### Scalability

Scalability is the capability of a system to grow and manage increased demand. Distributed systems that can continuously evolve in order to support the growing amount of work is considered to be scalable.

 - **Horizontal Scaling**: scale by adding more servers into your pool of resources. It is often easier to scale dynamically by adding more machines into the existing pool. E.g. [Cassandra](https://en.wikipedia.org/wiki/Apache_Cassandra) and [MongoDB](https://en.wikipedia.org/wiki/MongoDB) provide an easy way to scale horizontally by adding more machines to meet growing needs.

 - **Vertical Scaling**: scale by adding more power (CPU, RAM, Storage, etc.) to an existing server. It is usually limited to the capacity of a single server and scaling beyond that capacity often involves downtime and comes with an upper limit. E.g. MySQL.



### Reliability

 A distributed system is considered reliable if it keeps delivering its services even when one or several of its software or hardware components fail. A reliable distributed system achieves this through redundancy of both the software components and data. Obviously, redundancy has a cost and a reliable system has to pay that to achieve such resilience for services by eliminating every single point of failure.



### Availability

It is a simple measure of the percentage of time that a system, service, or a machine remains operational under normal conditions.

#### Reliability Vs. Availability

If a system is reliable, it is available. However, if it is available, it is not necessarily reliable. High reliability contributes to high availability, but it is possible to achieve a high availability even with an unreliable product by minimizing repair time and ensuring that spares are always available when they are needed. 


### Efficiency

Two standard measures of a system's efficiency are the response time and the throughput.


### Serviceability or Manageability

Another important consideration while designing a distributed system is how easy it is to operate and maintain. Serviceability or manageability is the simplicity and speed with which a system can be repaired or maintained.


## Load Balancing


The reason why we need load balancer includes,

 - it helps to spread the traffic across a cluster of servers
 - it keeps track of the status of all the resources while distributing requests

Load balancers usually sit between the client and the server, accepting incoming requests and distributing the traffic across multiple backend servers using various algorithms, thus a load balancer reduces individual server load and prevents any one application server from becoming a single point of failure.

To utilize full scalability and redundancy, we can try to balance the load at each layer of the system. 

### Benefits of Load Balancing

 - Users requests are immediately passed on to more readily available resources, instead of waiting for a single server to finish its previous tasks.
 - Less downtime and higher throughput.
 - Smart load balancer provides additional features, like predictive analytics that determine traffic bottlenecks before they happen.

### Load Balancing Algorithms

Load balancers consider two factors before forwarding a request to a backend server,

 - first ensure that the server they choose is actually responding appropriately to requests
 - use a pre-configured algorithm to select one from the set of healthy servers

To decide if a backend server is healthy or not, health check is conducted. "Health checks" regularly attempt to connect to backend servers to ensure that servers are listening.

A range of load balancing methods that are being used,

- Least Connection Method: directs traffic to the server with the fewest active connections
- Least Response Time Method: directs traffic to the server with the fewest active connections and the lowest average response time
- Least Bandwidth Method: selects the server that is currently serving the least amount of traffic
- Round Robin Method: cycles through a list of servers and sends each new request to the next server
- Weighted Round Robin Method: designed to better handle servers with different processing capacities
- IP Hash: a hash of the IP address of the client is calculated to redirect the request

### Redundant Load Balancers

A second load balancer can be connected to the first to form a cluster, to remove single point of failure.

## Caching

Caches take advantage of the locality of reference principle: recently requested data is likely to be requested again, and it will enable you to make vastly better use of the resource you already have. 

### Application server cache

Placing a cache directly on a request layer node enables the local storage of response data. However, if the request layer is expanded to multiple nodes, and the load balancer randomly distributes requests across the nodes, the same requests go to different nodes, thus increase cache misses. 

A solution would be global caches or distributed caches.

### Content Delivery (or Distribution) Network (CDN)

CDNs are a kind of cache that comes into play for sites serving large amounts of static media. It speeds up the page loading by hosting the static media closer to the clients. A normal setup is, the clients would request static assets from a CDN server that is close to them. If the resource can be found locally, it will be served. Otherwise, the CDN server would send the request to backend server for the client, then send it to the client and cache it locally.


### Cache Invalidation

While caching can speed up read requests, it requires maintenance to keep the cache and DB consistent. When data is updated in DB, Relevant cache needs to be invalidated.

 - Write-through cache: data is written into the cache and DB simultaneously, nothing will be lost in case of crash or other disruptions, user might experience higher latency for write requests.

 - Write-around cache: data is written into DB, bypassing the cache, it can reduce the cache being flooded with write requests (if writing data that is not likely to be read later, write-through schema will create cache entries anyway), but a write to recently read data would probably create a cache miss.

 - Write-back cache: data is written to cache alone, and completion is immediately confirmed to the client, write to DB is done later, it has low-latency and high-throughput for write intensive applications, but also has the risk of data loss in case of crash.

### Cache eviction policies

 - First in first out (FIFO)
 - Last in first out (LIFO)
 - Least recently used (LRU)
 - Most recently used (MRU)
 - Least frequently used (LFU)
 - Random replacement (RR)