## System Design Basics

Investing in scaling before it is needed is generally not a smart business proposition. However, some forethought into the design can save valuable time and resources in the future. 


## Key Characteristics of Distributed Systems

### Scalability

Scalability is the capability of a system to grow and manage increased demand. Distributed systems that can continuously evolve in order to support the growing amount of work is considered to be scalable.

 - **Horizontal Scaling**: scale by adding more servers into your pool of resources. It is often easier to scale dynamically by adding more machines into the existing pool. E.g. [Cassandra](https://en.wikipedia.org/wiki/Apache_Cassandra) and [MongoDB](https://en.wikipedia.org/wiki/MongoDB) provide an easy way to scale horizontally by adding more machines to meet growing needs.

 - **Vertical Scaling**: scale by adding more power (CPU, RAM, Storage, etc.) to an existing server. It is usually limited to the capacity of a single server and scaling beyond that capacity often involves downtime and comes with an upper limit. E.g. MySQL.



### Reliability

 A distributed system is considered reliable if it keeps delivering its services even when one or several of its software or hardware components fail. A reliable distributed system achieves this through redundancy of both the software components and data. Obviously, redundancy has a cost and a reliable system has to pay that to achieve such resilience for services by eliminating every single point of failure.



### Availability

It is a simple measure of the percentage of time that a system, service, or a machine remains operational under normal conditions.

#### Reliability Vs. Availability

If a system is reliable, it is available. However, if it is available, it is not necessarily reliable. High reliability contributes to high availability, but it is possible to achieve a high availability even with an unreliable product by minimizing repair time and ensuring that spares are always available when they are needed. 


### Efficiency

Two standard measures of a system's efficiency are the response time and the throughput.


### Serviceability or Manageability

Another important consideration while designing a distributed system is how easy it is to operate and maintain. Serviceability or manageability is the simplicity and speed with which a system can be repaired or maintained.


## Load Balancing


The reason why we need load balancer includes,

 - it helps to spread the traffic across a cluster of servers
 - it keeps track of the status of all the resources while distributing requests

Load balancers usually sit between the client and the server, accepting incoming requests and distributing the traffic across multiple backend servers using various algorithms, thus a load balancer reduces individual server load and prevents any one application server from becoming a single point of failure.

To utilize full scalability and redundancy, we can try to balance the load at each layer of the system. 

### Benefits of Load Balancing

 - Users requests are immediately passed on to more readily available resources, instead of waiting for a single server to finish its previous tasks.
 - Less downtime and higher throughput.
 - Smart load balancer provides additional features, like predictive analytics that determine traffic bottlenecks before they happen.

### Load Balancing Algorithms

Load balancers consider two factors before forwarding a request to a backend server,

 - first ensure that the server they choose is actually responding appropriately to requests
 - use a pre-configured algorithm to select one from the set of healthy servers

To decide if a backend server is healthy or not, health check is conducted. "Health checks" regularly attempt to connect to backend servers to ensure that servers are listening.

A range of load balancing methods that are being used,

- Least Connection Method: directs traffic to the server with the fewest active connections
- Least Response Time Method: directs traffic to the server with the fewest active connections and the lowest average response time
- Least Bandwidth Method: selects the server that is currently serving the least amount of traffic
- Round Robin Method: cycles through a list of servers and sends each new request to the next server
- Weighted Round Robin Method: designed to better handle servers with different processing capacities
- IP Hash: a hash of the IP address of the client is calculated to redirect the request

### Redundant Load Balancers

A second load balancer can be connected to the first to form a cluster, to remove single point of failure.

## Caching

Caches take advantage of the locality of reference principle: recently requested data is likely to be requested again, and it will enable you to make vastly better use of the resource you already have. 

### Application server cache

Placing a cache directly on a request layer node enables the local storage of response data. However, if the request layer is expanded to multiple nodes, and the load balancer randomly distributes requests across the nodes, the same requests go to different nodes, thus increase cache misses. 

A solution would be global caches or distributed caches.

### Content Delivery (or Distribution) Network (CDN)

CDNs are a kind of cache that comes into play for sites serving large amounts of static media. It speeds up the page loading by hosting the static media closer to the clients. A normal setup is, the clients would request static assets from a CDN server that is close to them. If the resource can be found locally, it will be served. Otherwise, the CDN server would send the request to backend server for the client, then send it to the client and cache it locally.


### Cache Invalidation

While caching can speed up read requests, it requires maintenance to keep the cache and DB consistent. When data is updated in DB, Relevant cache needs to be invalidated.

 - Write-through cache: data is written into the cache and DB simultaneously, nothing will be lost in case of crash or other disruptions, user might experience higher latency for write requests.

 - Write-around cache: data is written into DB, bypassing the cache, it can reduce the cache being flooded with write requests (if writing data that is not likely to be read later, write-through schema will create cache entries anyway), but a write to recently read data would probably create a cache miss.

 - Write-back cache: data is written to cache alone, and completion is immediately confirmed to the client, write to DB is done later, it has low-latency and high-throughput for write intensive applications, but also has the risk of data loss in case of crash.

### Cache eviction policies

 - First in first out (FIFO)
 - Last in first out (LIFO)
 - Least recently used (LRU)
 - Most recently used (MRU)
 - Least frequently used (LFU)
 - Random replacement (RR)


## Data Partitioning

Data partitioning is a technique to break up a big database (DB) into many smaller parts. The justification for data partitioning is that, after a certain scale point, it is cheaper and more feasible to scale horizontally by adding more machines than to grow it vertically by adding beefier servers.

### Partitioning Methods

#### Horizontal partitioning

 1. put different rows into different tables
 2. also called range based partitioning or data sharding
 3. might lead to unbalanced servers if the range used for partitioning is not chosen carefully, e.g. some rows might be hotter and requested more frequently

#### Vertical partitioning

 1. divide data to store tables related to a specific feature in their own server
 2. straightforward to implement and has low impacts on the application

#### Directory based partitioning

 1. create a lookup service which knows your current partitioning scheme and abstracts it away from the DB access code
 2. can perform tasks like adding servers to the DB pool or changing our partitioning scheme without having an impact on the application

### Partitioning Criteria

#### Key or hash-based partitioning

 1. apply a hash function to some key attributes of the entity we are storing, that yields the partition number
 2. it effectively fixes the total number of DB servers, which means it requires redistribution and downtime when changing the hash function or adding new servers
 3. a workaround is to use consistent hashing

#### List partitioning

 1. each partition is assigned a list of values
 2. when inserting a new record, we check which partition contains the key of the new record and store it there

#### Round-robin partitioning

Simple strategy that ensures uniform data distribution.

#### Composite partitioning

We combine any of the above partitioning schemes to devise a new scheme.

### Common Problems of Data Partitioning

On a partitioned database, there are certain extra constraints on the different operations that can be performed.

#### Joins and denormalization

 1. it is often not feasible to perform joins that span database partitions
 2. a common workaround for this problem is to denormalize the database so that queries that previously required joins can be performed from a single table

#### Referential integrity

 1. trying to enforce data integrity constraints such as foreign keys in a partitioned database can be extremely difficult
 2. most of RDBMS do not support foreign keys constraints across databases on different database servers
 3. applications that require referential integrity on partitioned databases often have to enforce it in application code

#### Rebalancing

 1. there could be many reasons we have to change our partitioning scheme
 2. rebalancing without incurring downtime is extremely difficult
 3. directory based partitioning does make rebalancing more palatable, but increases the complexity of the system and a new single point of failure (e.g. lookup service)

### Indexes

Indexes can be created using one or more columns of a database table, providing the basis for both rapid random lookups and efficient access of ordered records. Simply saying, an index is a data structure that can be perceived as a table of contents that points us to the location where actual data lives.

An index can dramatically speed up data retrieval but may itself be large due to the additional keys, which slow down data insertion & update. When adding rows or making updates to existing rows for a table with an active index, we not only have to write the data but also have to update the index. This will decrease the write performance.

### Proxies

A proxy server is an intermediate server between the client and the back-end server. Typically, proxies are used to filter requests, log requests, or sometimes transform requests. Another advantage of a proxy server is that its cache can serve a lot of requests.

#### Proxy Server Types

Proxies can reside on the client’s local server or anywhere between the client and the remote servers.

##### Open proxy

An open proxy is a proxy server that is accessible by any Internet user. There are two famous open proxy types:

 1. Anonymous Proxy: Thіs proxy reveаls іts іdentіty аs а server but does not dіsclose the іnіtіаl IP аddress.
 2. Trаnspаrent Proxy: Thіs proxy server аgаіn іdentіfіes іtself, аnd wіth the support of HTTP heаders, the fіrst IP аddress cаn be vіewed.


##### Reverse proxy 

A reverse proxy retrieves resources on behalf of a client from one or more servers.


### Redundancy and Replication

Redundancy is the duplication of critical components or functions of a system with the intention of increasing the reliability of the system, usually in the form of a backup or fail-safe, or to improve actual system performance.

Redundancy plays a key role in removing the single points of failure in the system and provides backups if needed in a crisis.

Replication means sharing information to ensure consistency between redundant resources. Replication is widely used in many database management systems (DBMS), usually with a primary-replica relationship between the original and the copies.

### SQL vs NoSQL

In the world of databases, there are two main types of solutions: SQL and NoSQL. Relational databases are structured and have predefined schemas, while non-relational databases are unstructured, distributed, and have a dynamic schema.

#### SQL

Relational databases store data in rows and columns.

#### NoSQL

Common types of NoSQL:

##### Key-Value Stores

Data is stored in an array of key-value pairs, e.g. Redis, Voldemort, and Dynamo.

##### Document Databases

Data is stored in documents and these documents are grouped together in collections, they can have totally different structures. E.g. CouchDB and MongoDB.

##### Wide-Column Databases

In columnar databases we have column families, which are containers for rows. We don’t need to know all the columns up front and each row doesn’t have to have the same number of columns. E.g. Cassandra and HBase.

##### Graph Databases

Used to store data whose relations are best represented in a graph. E.g. Neo4J and InfiniteGraph. Data is saved in graph structures with nodes (entities), properties (information about the entities), and lines (connections between the entities).

#### High level differences between SQL and NoSQL

##### Storage

SQL stores data in tables where each row represents an entity and each column represents a data point about that entity. NoSQL databases have different data storage models.

##### Schema

In SQL, each record conforms to a fixed schema. The schema can be altered later, but it involves modifying the whole database and going offline. In NoSQL, schemas are dynamic.

##### Querying

SQL databases use SQL (structured query language) for defining and manipulating the data. In a NoSQL database, queries are focused on a collection of documents. Sometimes it is also called UnQL (Unstructured Query Language).

##### Scalability

SQL databases are vertically scalable, which can get very expensive. It's possible to horizontally scale a relational database, but it's really challenging. 

NoSQL databases are horizontally scalable. We can add more servers easily in our NoSQL database infrastructure to handle a lot of traffic. And a lot of NoSQL technologies also distribute data across servers automatically.

##### Reliability or ACID Compliancy (Atomicity, Consistency, Isolation, Durability)

The vast majority of relational databases are ACID compliant. Most of the NoSQL solutions sacrifice ACID compliance for performance and scalability.

#### SQL VS. NoSQL - Which one to use?

##### Reasons to use SQL database

 1. We need to ensure ACID compliance. Generally, NoSQL databases sacrifice ACID compliance for scalability and processing speed.
 2. Your data is structured and unchanging. 

##### Reasons to use NoSQL database

 1. Storing large volumes of data that often have little to no structure.
 2. Making the most of cloud computing and storage. Cloud-based storage is an excellent cost-saving solution but requires data to be easily spread across multiple servers to scale up.
 3. Rapid development. NoSQL is extremely useful for rapid development as it doesn’t need to be prepped ahead of time.

### Consistent Hashing

#### Background

 - Data partitioning: It is the process of distributing data across a set of servers. It improves the scalability and performance of the system.
 - Data replication: It is the process of making multiple copies of data and storing them on different servers. It improves the availability and durability of the data across the system.

 A carefully designed scheme for partitioning and replicating the data enhances the performance, availability, and reliability of the system. Consistent Hashing efficiently solves the problem of data partitioning and replication.

#### What is data partitioning?

A naive way, mapping data to a node with a hash function can help to distribute the data, but one problem is the size of the server cluster is effectively fixed. When adding/removing servers, we need to redistribute the data, which hurts the system's availability.

#### Consistent Hashing to the rescue

Consistent Hashing maps data to physical nodes and **ensures that only a small set of keys move when servers are added or removed**. Consistent Hashing stores the data managed by a distributed system in a ring. Each node in the ring is assigned a range of data. The start of the range is called a token. This means that each node will be assigned one token.

<img src="https://i.ibb.co/Fnx3kvT/Screen-Shot-2021-08-03-at-12-51-12-am.png" alt="Screen-Shot-2021-08-03-at-12-51-12-am">

In this case,

<img src="https://i.ibb.co/jMmm7x4/Screen-Shot-2021-08-03-at-12-55-17-am.png" alt="Screen-Shot-2021-08-03-at-12-55-17-am">

Whenever the system needs to read or write data, 

 1. it performs is to apply the MD5 hashing algorithm to the key.
 2. the result determines within which range the data lies and hence, on which node the data will be stored

It works great when a node is added or removed from the ring, as in these cases, since only the next node is affected. However, this scheme can result in non-uniform data and load distribution.

#### Virtual nodes

To efficiently handle the cases where the number of servers changes, Consistent Hashing makes use of virtual nodes (or Vnodes). Assigning one takon to each node (a consecutive hash range) can have issues,

 1. Adding or removing nodes will result in recomputing the tokens causing a significant administrative overhead.
 2. If the data is not evenly distributed, some nodes can become hotspots.
 3. Since each node’s data might be replicated (for fault-tolerance) on a fixed number of other nodes, when we need to rebuild a node, only its replica nodes can provide the data. This puts a lot of pressure on the replica nodes.

To solve these issues, instead of assigning a single token to a node, the hash range is divided into multiple smaller ranges, and each physical node is assigned several of these smaller ranges. Each small range is a virtual node, and many tokens would be assigned to one node.

Practically, Vnodes are randomly distributed across the cluster and are generally non-contiguous. Some powerful servers might hold more Vnodes than others. 

<img src="https://i.ibb.co/hg7XdyL/Screen-Shot-2021-08-03-at-9-53-58-am.png" alt="Screen-Shot-2021-08-03-at-9-53-58-am">

#### Advantages of Vnodes

 1. It speeds up the rebalancing process after adding or removing nodes. It receives or gives Vnodes to a subset of existing servers. And when rebuild a node, multiple servers can provide data instead of just one.
 2. Vnodes make it easier to maintain a cluster containing heterogeneous machines.
 3. Since Vnodes help assign smaller ranges to each physical node, this decreases the probability of hotspots.

#### Data replication using Consistent Hashing

Each key is assigned to a coordinator node (generally the first node that falls in the hash range), which first stores the data locally and then replicates it to `N-1` clockwise successor nodes on the ring.

<img src="https://i.ibb.co/xLrfrDJ/Screen-Shot-2021-08-03-at-10-03-05-am.png" alt="Screen-Shot-2021-08-03-at-10-03-05-am">

#### Consistent Hashing in System Design Interviews

Any distributed system that needs to scale up or down or wants to achieve high availability through data replication can utilize Consistent Hashing.

### Long-Polling vs WebSockets vs Server-Sent Events

#### Ajax Polling

The basic idea is that the client repeatedly polls (or requests) a server for data. The client makes a request and waits for the server to respond with data. If no data is available, an empty response is returned.

The problem with Polling is that the client has to keep asking the server for any new data. As a result, a lot of responses are empty, creating HTTP overhead.

#### HTTP Long-Polling

With Long-Polling, the client requests information from the server exactly as in normal polling, but with the expectation that the server may not respond immediately, that's why it's also called "hanging GET". 

 - Instead of sending an empty response, the server holds the request and waits until some data becomes available.
 - Once the data becomes available, a full response is sent to the client.
 - The client typically sends a new long-poll request, either immediately upon receiving a response or after a pause to allow an acceptable latency period.
 - Each Long-Poll request has a timeout.

#### WebSockets

It provides a persistent connection between a client and a server that both parties can use to start sending data at any time. The WebSocket protocol enables communication between a client and a server with lower overheads, facilitating real-time data transfer from and to the server.

#### Server-Sent Events (SSEs)

Under SSEs the client establishes a persistent and long-term connection with the server. The server uses this connection to send data to a client. But if the client wants to send data to the server, it would require the use of another technology/protocol to do so.