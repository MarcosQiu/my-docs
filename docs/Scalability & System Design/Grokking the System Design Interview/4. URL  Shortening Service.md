## Requirements and Goals of the System

### Functional Requirements

 1. Given a URL, our service should generate a shorter and unique alias of it. It should be short enough to be easily copied and pasted.
 2. When users access a short link, our service should redirect them to the original link.
 3. Users should optionally be able to pick a custom short link for their URL.
 4. Links will expire after a standard default timespan. Users should be able to specify the expiration time.

### Non-Functional Requirements

 1. The system should be highly available. 
 2. URL redirection should happen in real-time with minimal latency.
 3. Shortened links should not be guessable (not predictable).

### Extended Requirements

 1. Analytics; e.g., how many times a redirection happened?
 2. Our service should also be accessible through REST APIs by other services.

## Questions That Could be Asked

 - Will the short urls expire?
 - Will the users be able to select customised urls?
 - What's the read-write ratio?
 - How many shortening url requests per second?

## Capacity Estimation and Constraints

 1. read-write ration would be `100:1`
 2. assume `500M` new shortened url per month, read requests would be `500M * 100 = 5e10` per month
 3. so per second, there will be `500M / 30 / 24 / 3600`, which is around `200`
 4. and number of reads per second will be around `20K`
 5. assume we store the urls for 5 years, so we need to store in total `500M * 12 * 5 = 3e10` urls
 6. assume `0.5KB` per url, so that would be `15TB` of storage
 7. for write operations, the bandwidth requirement is `200/s * 0.5KB = 100KB/s`, the bandwidth requirement for read is `100KB/s * 100 = 10MB/s`
 8. so everyday, there will be `10MB/s * 3600 * 24 = 860GB` data being read, assume 20% of them generate 80% of the traffic, and we want to keep them in cache, that would require `860GB * 20% = 172GB`.

## Database Design

Only need to store original url, shortened url, expiry date and the user id who created the url for each entries. As the data size can be huge, and no requirement for many table joins, can choose NoSQL database, which is easier to scale. 

## Basic System Design and Algorithm

### Encoding actual URL

A naive way is to apply hash function on the origin url, and then do base64 encoding. If we use 6 character-long encoded key, we could have `6.87e10` unique keys, given that for the whole 5 years, there would be around `3e10` urls, 6 characters should be enough. 

Hash function like MD5, generates 128-bit hash value. In the encoded key, each character could encode 6 bit hash value (as `2^6=64`). So we only need to pick 6 characters out of the encoded hash.

Potential problem:

 1. different users encoding the same url might get the same result
 2. url may looks different after being encoded, but they are probably the same

Potential solution:

 1. append a increasing sequential number after the url, and then calculate the hash
 2. we can always encode the url first being calculating hash

<img src="https://i.ibb.co/48NvDmN/Screen-Shot-2021-08-05-at-10-18-28-am.png" alt="Screen-Shot-2021-08-05-at-10-18-28-am">

### Keys generating services

Another option is to generate the keys offline and store the six-character long keys in a key DB.

#### Potential problem and solution

 - **Concurrency issue**: We need to prevent the situation that key being used multiple times. once a key is used, it should be marked as used. For simplicity, as soon as KGS loads some keys in memory, it can move them to the used keys table. This ensures each server gets unique keys. Even the service gets restarted, losing some of the keys is acceptable. It also needs some sort of lock to make sure that KGS is not giving the same key to different servers.
 - **Key-DB size**: Assume we need 6 bytes to store a six-character long key, total size for the key-DB is `6 * 6.87e10 = 412GB`.
 - **Single point failure**: KGS could be a single point failure. To solve it, we could just have a standby KGS replica.
 - **Each app server cache some keys from key-DB**: This can be done and better for the performance.
 - **Impose size limits on custom aliases**: It's desirable to have a limit on how long the aliases could be. For example we could use 16 characters limit.


## Data Partitioning and Replication

Since we need 15TB to store all the records, we need to partition the data.

One way to do is using range based partitioning. For example, all the urls with key starting with `A` goes to a partition, etc. However, this could lead to unbalanced split and overloaded partitions.

A solution to this problem is using consistent hash.

## Cache

### Cache size

The cache size would be around `172GB` as we calcuated before.

### Cache eviction policy 

Using LRU policy makes sence.

### How can each cache replica be updated

Every time there's a cache miss, we query the DB to fetch the url, and send the result to cache servers to update the cache.

## Load Balancer (LB)

We could add LBs in front of the components that need a cluster of server, for example

 1. the app server clusters
 2. the cache servers and clusters
 3. the DB clusters

LB will create a new single point of failure, to solve that, we need to add a standby replica for each, just in case that the LB goes down.


## Purging or DB cleanup

It's a good idea to delete expired urls. One option is to have a standalone service, searching and removing expired entries on the background, which might put a lot pressure a the system.

Another way is to do lasy clean up. Every time a user query for a expired key, we remove the key and return 404 error. Also, on top of that, we could scan and clean expired keys periodically. 

<img src="https://i.ibb.co/kXLcyCR/Screen-Shot-2021-08-05-at-12-46-49-pm.png" alt="Screen-Shot-2021-08-05-at-12-46-49-pm">